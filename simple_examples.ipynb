{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import seaborn\n",
    "\n",
    "\n",
    "#Test variables for regression (simple addition function)\n",
    "train_data = (np.random.randn(1000,2)*10).astype(np.int16).astype(np.float64)\n",
    "train_labels = np.array([ x + y +40 for [x,y] in train_data]).reshape((train_data.shape[0],1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple L2 linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "MSE: 7986.79925608, Epoch: 0, W: [[-1.25173542]\n",
      " [-0.08114633]], bias: 0\n",
      "MSE: 6677.10108167, Epoch: 200, W: [[ 10.23061513]\n",
      " [ 25.33789946]], bias: 0\n",
      "MSE: 8189.18350059, Epoch: 400, W: [[ 25.5890907 ]\n",
      " [ -6.20034049]], bias: 0\n",
      "MSE: 1621.21187758, Epoch: 600, W: [[ -7.80507365]\n",
      " [ 23.79066985]], bias: 0\n",
      "MSE: 4926.87746383, Epoch: 800, W: [[ 13.90428456]\n",
      " [  0.52979456]], bias: 0\n",
      "MSE: 0.0, Epoch: 1000, W: [[ -6.69665694]\n",
      " [-16.22781389]], bias: 0\n",
      "MSE: 0.0, Epoch: 1200, W: [[ -6.69665694]\n",
      " [-16.22781389]], bias: 0\n",
      "MSE: 0.0, Epoch: 1400, W: [[ -6.69665694]\n",
      " [-16.22781389]], bias: 0\n",
      "MSE: 0.0, Epoch: 1600, W: [[ -6.69665694]\n",
      " [-16.22781389]], bias: 0\n",
      "MSE: 0.0, Epoch: 1800, W: [[ -6.69665694]\n",
      " [-16.22781389]], bias: 0\n"
     ]
    }
   ],
   "source": [
    "n_samples = train_data.shape[0]\n",
    "batch_size = 100\n",
    "feature_length = train_data.shape[1]\n",
    "epochs = 2000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    #Input matrix placeholder\n",
    "    X = tf.placeholder(tf.float64)\n",
    "    #Labels placeholder\n",
    "    Y_ = tf.placeholder(tf.float64)\n",
    "    \n",
    "    #Weights\n",
    "    W = tf.Variable(np.random.randn(feature_length,1),dtype=tf.float64)\n",
    "    #Bias\n",
    "    b = tf.Variable(0, dtype= tf.float64)\n",
    "    \n",
    "    #The prediction\n",
    "    pred = tf.matmul(X,W)\n",
    "    \n",
    "    #L2 regularization\n",
    "    regularizers = tf.nn.l2_loss(W) \n",
    "    \n",
    "    #Cost function, mean squared error\n",
    "    cost = tf.reduce_sum(tf.pow(Y_ - pred, 2))/2./n_samples + regularizers\n",
    "    \n",
    "    #Use the gradient descent optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.0001)\n",
    "    #Define the train step\n",
    "    train_step = optimizer.minimize(cost)\n",
    "    \n",
    "    #Initialize all variables\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "\n",
    "    #Train by epochs\n",
    "    for i in range(0,epochs):\n",
    "        \n",
    "        #Do train step in batches, we do not want to multiply huge matrices\n",
    "        for batch_step in np.arange(0, n_samples, batch_size):\n",
    "            train_batch = train_data[i:i+batch_size]\n",
    "            train_label_batch =  train_labels[i:i+batch_size]\n",
    "            sess.run(train_step, feed_dict={X : train_batch, Y_ : train_label_batch})\n",
    "            \n",
    "        #Print epoch information\n",
    "        if(i%200 == 0):\n",
    "            c = sess.run(cost, feed_dict={X : train_batch, Y_ : train_label_batch})\n",
    "            weights = sess.run(W, feed_dict={X : train_batch, Y_ : train_label_batch})\n",
    "            bias =  sess.run(b, feed_dict={X : train_batch, Y_ : train_label_batch})\n",
    "            print(\"MSE: %s, Epoch: %d, W: %s, bias: %d\" %(str(c), i, str(weights), bias) ,end=\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = train_data.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    n_samples = train_data.shape[0]\n",
    "    feature_length = train_data.shape[1]\n",
    "    \n",
    "    X = tf.placeholder(tf.float64)\n",
    "    Y_ = tf.placeholder(tf.float64)\n",
    "    \n",
    "    W = tf.Variable(np.random.randn(feature_length,1),dtype=tf.float64)\n",
    "    b = tf.Variable(0, dtype= tf.float64)\n",
    "    \n",
    "\n",
    "    pred = tf.matmul(X,W)\n",
    "    \n",
    "    #L2 regularization\n",
    "    regularizers = tf.nn.l2_loss(W) \n",
    "    \n",
    "    M = tf.sub(Y_, pred)\n",
    "    cost = tf.reduce_sum(tf.matmul(M, M, transpose_b=True))/2./n_samples + regularizers\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.0001)\n",
    "    \n",
    "    train_step = optimizer.minimize(cost)\n",
    "    \n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    for i in range(0,2000):\n",
    "        \n",
    "\n",
    "        for batch_step in np.arange(0, n_samples, batch_size):\n",
    "            train_batch = train_data[i:i+batch_size]\n",
    "            train_label_batch =  train_labels[i:i+batch_size]\n",
    "            sess.run(train_step, feed_dict={X : train_batch, Y_ : train_label_batch})\n",
    "            \n",
    "\n",
    "        if(i%200 == 0):\n",
    "            c = sess.run(cost, feed_dict={X : train_batch, Y_ : train_label_batch})\n",
    "            weights = sess.run(W, feed_dict={X : train_batch, Y_ : train_label_batch})\n",
    "            bias =  sess.run(b, feed_dict={X : train_batch, Y_ : train_label_batch})\n",
    "            print(\"MSE: %s, Epoch: %d, W: %s, bias: %d\" %(str(c), i, str(weights), bias) ,end=\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
