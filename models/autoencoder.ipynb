{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Artificial dataset, lets say XOR\n",
    "def xor():\n",
    "    X = np.array([[0,1], [1,0], [1,1],[0,0]], dtype=float)\n",
    "    Y = np.array([1,1,0,0], dtype=float).reshape((-1,1))\n",
    "    return X,Y\n",
    "\n",
    "def square_function():\n",
    "    X = []\n",
    "    Y = []\n",
    "    for x in range(10):\n",
    "        X.append(x)\n",
    "        Y.append(x**2)\n",
    "    return np.array(X,dtype=np.float32).reshape(-1,1),np.array(Y, dtype=np.float32).reshape(-1,1)\n",
    "\n",
    "def high_dim():\n",
    "    X = []\n",
    "    Y = []\n",
    "    for x in range(100):\n",
    "        X.append([x, x+3, x+4, x+5, x+10, x+20, x+60])\n",
    "    return np.array(X,dtype=np.float32).reshape(-1,7), None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple autoencoder\n",
    "\n",
    "So basically the only thing that is changing is the loss function, we want to get the same output as the input. This can be used to reduce the dimensionality of the input (encoding) with hidden layers, then we get the input back by decoding later, this is what the network is training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 20)\n",
      "(?, 20)\n",
      "(?, 4)\n",
      "(?, 20)\n",
      "(?, 7)\n"
     ]
    }
   ],
   "source": [
    "#input\n",
    "\n",
    "X,Y = high_dim()\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "#Helper functions\n",
    "def fully_connected(x, size, activation=tf.nn.relu):\n",
    "    b = tf.Variable(tf.zeros([size]))\n",
    "    weights = tf.Variable(tf.truncated_normal([x.get_shape()[1].value, size]))\n",
    "    return activation(b + tf.matmul(x, weights))\n",
    "\n",
    "\n",
    "INPUT_SHAPE = X.shape\n",
    "\n",
    "x = tf.placeholder(\"float\", [None,X.shape[1]])\n",
    "\n",
    "#simple neural network to solve the xor problem\n",
    "\n",
    "h1 = fully_connected(x, 20, activation=tf.nn.tanh)\n",
    "print(h1.get_shape())\n",
    "\n",
    "h2 = fully_connected(h1, 20)\n",
    "print(h2.get_shape())\n",
    "\n",
    "h3 = fully_connected(h2, 4)\n",
    "print(h3.get_shape())\n",
    "\n",
    "h4 = fully_connected(h3, 20)\n",
    "print(h4.get_shape())\n",
    "\n",
    "out = fully_connected(tf.nn.dropout(h4, 0.5), X.shape[1])\n",
    "print(out.get_shape())\n",
    "\n",
    "loss = tf.reduce_mean((out-x)**2)\n",
    "tf.scalar_summary(\"loss\", loss)\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.1)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Epoch Loss: 7357.2871 \n",
      "40. Epoch Loss: 3047.4851 \n",
      "80. Epoch Loss: 2939.5378 \n",
      "120. Epoch Loss: 3175.4475 \n",
      "160. Epoch Loss: 2284.9050 \n",
      "200. Epoch Loss: 2348.4766 \n",
      "240. Epoch Loss: 2704.4995 \n",
      "280. Epoch Loss: 2209.0679 \n",
      "320. Epoch Loss: 2424.2141 \n",
      "360. Epoch Loss: 1861.2480 \n",
      "400. Epoch Loss: 2161.4360 \n",
      "440. Epoch Loss: 2320.6472 \n",
      "480. Epoch Loss: 2161.1194 \n",
      "520. Epoch Loss: 2184.9412 \n",
      "560. Epoch Loss: 2203.8164 \n",
      "600. Epoch Loss: 2038.0466 \n",
      "640. Epoch Loss: 2097.0957 \n",
      "680. Epoch Loss: 2111.7097 \n",
      "720. Epoch Loss: 2093.4980 \n",
      "760. Epoch Loss: 2078.1555 \n",
      "800. Epoch Loss: 2049.7087 \n",
      "840. Epoch Loss: 2054.0583 \n",
      "880. Epoch Loss: 2039.0243 \n",
      "920. Epoch Loss: 2036.1897 \n",
      "960. Epoch Loss: 2034.2491 \n",
      "1000. Epoch Loss: 2034.1201 \n",
      "1040. Epoch Loss: 2034.1320 \n",
      "1080. Epoch Loss: 2034.0948 \n",
      "1120. Epoch Loss: 2034.1000 \n",
      "1160. Epoch Loss: 2034.0997 \n",
      "1200. Epoch Loss: 2034.0983 \n",
      "1240. Epoch Loss: 2034.0994 \n",
      "1280. Epoch Loss: 2034.1021 \n",
      "1320. Epoch Loss: 2034.1067 \n",
      "1360. Epoch Loss: 2034.1056 \n",
      "1400. Epoch Loss: 2034.1049 \n",
      "1440. Epoch Loss: 2034.1045 \n",
      "1480. Epoch Loss: 2034.1042 \n",
      "1520. Epoch Loss: 2034.1040 \n",
      "1560. Epoch Loss: 2034.1040 \n",
      "1600. Epoch Loss: 2034.1040 \n",
      "1640. Epoch Loss: 2034.1040 \n",
      "1680. Epoch Loss: 2034.1040 \n",
      "1720. Epoch Loss: 2034.1040 \n",
      "1760. Epoch Loss: 2034.1040 \n",
      "1800. Epoch Loss: 2034.1040 \n",
      "1840. Epoch Loss: 2034.1040 \n",
      "1880. Epoch Loss: 2034.1040 \n",
      "1920. Epoch Loss: 2034.1040 \n",
      "1960. Epoch Loss: 2034.1040 \n",
      "2000. Epoch Loss: 2034.1040 \n",
      "2040. Epoch Loss: 2034.1040 \n",
      "2080. Epoch Loss: 2034.1040 \n",
      "2120. Epoch Loss: 2034.1040 \n",
      "2160. Epoch Loss: 2034.1040 \n",
      "2200. Epoch Loss: 2034.1040 \n",
      "2240. Epoch Loss: 2034.1040 \n",
      "2280. Epoch Loss: 2034.1040 \n",
      "2320. Epoch Loss: 2034.1040 \n",
      "2360. Epoch Loss: 2034.1040 \n",
      "2400. Epoch Loss: 2034.1040 \n",
      "2440. Epoch Loss: 2034.1040 \n",
      "2480. Epoch Loss: 2034.1040 \n",
      "2520. Epoch Loss: 2034.1040 \n",
      "2560. Epoch Loss: 2034.1040 \n",
      "2600. Epoch Loss: 2034.1040 \n",
      "2640. Epoch Loss: 2034.1040 \n",
      "2680. Epoch Loss: 2034.1040 \n",
      "2720. Epoch Loss: 2034.1040 \n",
      "2760. Epoch Loss: 2034.1040 \n",
      "2800. Epoch Loss: 2034.1040 \n",
      "2840. Epoch Loss: 2034.1040 \n",
      "2880. Epoch Loss: 2034.1040 \n",
      "2920. Epoch Loss: 2034.1040 \n",
      "2960. Epoch Loss: 2034.1040 \n",
      "3000. Epoch Loss: 2034.1040 \n",
      "3040. Epoch Loss: 2034.1040 \n",
      "3080. Epoch Loss: 2034.1040 \n"
     ]
    }
   ],
   "source": [
    "init =  tf.initialize_all_variables()\n",
    "\n",
    "num_epochs=10000\n",
    "#merge = tf.merge_all_summaries()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    writer = tf.train.SummaryWriter(\"./summary\", sess.graph)\n",
    "    sess.run(init)\n",
    "    #sess.run(merge)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(X.shape[0]//batch_size):\n",
    "            feed_dict = {\n",
    "                x : X[batch*batch_size:batch*batch_size + batch_size],\n",
    "            }\n",
    "            _, lossVal = sess.run([train_step, loss], feed_dict=feed_dict)\n",
    "        #writer.add_summary(merged, epoch)\n",
    "        if(epoch%40 == 0):\n",
    "            print(\"%d. Epoch Loss: %.4f \"  %(epoch, lossVal))\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
