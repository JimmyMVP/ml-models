{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Artificial dataset, lets say XOR\n",
    "def xor():\n",
    "    X = np.array([[0,1], [1,0], [1,1],[0,0]], dtype=float)\n",
    "    Y = np.array([1,1,0,0], dtype=float).reshape((-1,1))\n",
    "    return X,Y\n",
    "\n",
    "def square_function():\n",
    "    X = []\n",
    "    Y = []\n",
    "    for x in range(10):\n",
    "        X.append(x)\n",
    "        Y.append(x**2)\n",
    "    return np.array(X,dtype=np.float32).reshape(-1,1),np.array(Y, dtype=np.float32).reshape(-1,1)\n",
    "\n",
    "def high_dim():\n",
    "    X = []\n",
    "    Y = []\n",
    "    for x in range(100):\n",
    "        X.append([x, x+3, x+4, x+5, x+10, x+20, x+60])\n",
    "    return np.array(X,dtype=np.float32).reshape(-1,7), None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple autoencoder\n",
    "\n",
    "So basically the only thing that is changing is the loss function, we want to get the same output as the input. This can be used to reduce the dimensionality of the input (encoding) with hidden layers, then we get the input back by decoding later, this is what the network is training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 20)\n",
      "(?, 20)\n",
      "(?, 4)\n",
      "(?, 20)\n",
      "(?, 7)\n"
     ]
    }
   ],
   "source": [
    "#input\n",
    "\n",
    "X,Y = high_dim()\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "#Helper functions\n",
    "def fully_connected(x, size, activation=tf.nn.relu):\n",
    "    b = tf.Variable(tf.zeros([size]))\n",
    "    weights = tf.Variable(tf.truncated_normal([x.get_shape()[1].value, size]))\n",
    "    return activation(b + tf.matmul(x, weights))\n",
    "\n",
    "\n",
    "INPUT_SHAPE = X.shape\n",
    "\n",
    "x = tf.placeholder(\"float\", [None,X.shape[1]])\n",
    "\n",
    "#simple neural network to solve the xor problem\n",
    "\n",
    "h1 = fully_connected(x, 20, activation=tf.nn.tanh)\n",
    "print(h1.get_shape())\n",
    "\n",
    "h2 = fully_connected(h1, 20)\n",
    "print(h2.get_shape())\n",
    "\n",
    "h3 = fully_connected(h2, 4)\n",
    "print(h3.get_shape())\n",
    "\n",
    "h4 = fully_connected(h3, 20)\n",
    "print(h4.get_shape())\n",
    "\n",
    "out = fully_connected(tf.nn.dropout(h4, 0.5), X.shape[1])\n",
    "print(out.get_shape())\n",
    "\n",
    "loss = tf.reduce_mean((out-x)**2)\n",
    "tf.scalar_summary(\"loss\", loss)\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.1)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Epoch Loss: 6731.2368 \n",
      "40. Epoch Loss: 4527.8496 \n",
      "80. Epoch Loss: 4075.4465 \n",
      "120. Epoch Loss: 4126.0259 \n",
      "160. Epoch Loss: 3960.4563 \n",
      "200. Epoch Loss: 3520.7549 \n",
      "240. Epoch Loss: 4118.2285 \n",
      "280. Epoch Loss: 3541.5635 \n",
      "320. Epoch Loss: 3261.5769 \n",
      "360. Epoch Loss: 3412.9380 \n",
      "400. Epoch Loss: 3228.7542 \n",
      "440. Epoch Loss: 3270.4038 \n",
      "480. Epoch Loss: 3346.4084 \n",
      "520. Epoch Loss: 3223.3364 \n",
      "560. Epoch Loss: 3267.9849 \n",
      "600. Epoch Loss: 3192.3806 \n",
      "640. Epoch Loss: 3234.2644 \n",
      "680. Epoch Loss: 3198.5796 \n",
      "720. Epoch Loss: 3203.4055 \n",
      "760. Epoch Loss: 3174.9805 \n",
      "800. Epoch Loss: 3164.3376 \n",
      "840. Epoch Loss: 3160.4604 \n",
      "880. Epoch Loss: 3159.6013 \n",
      "920. Epoch Loss: 3159.1167 \n",
      "960. Epoch Loss: 3159.0447 \n",
      "1000. Epoch Loss: 3159.0198 \n",
      "1040. Epoch Loss: 3159.0156 \n",
      "1080. Epoch Loss: 3159.0139 \n",
      "1120. Epoch Loss: 3159.0149 \n",
      "1160. Epoch Loss: 3159.0134 \n",
      "1200. Epoch Loss: 3159.0142 \n",
      "1240. Epoch Loss: 3159.0193 \n",
      "1280. Epoch Loss: 3159.0186 \n",
      "1320. Epoch Loss: 3159.0178 \n",
      "1360. Epoch Loss: 3159.0173 \n",
      "1400. Epoch Loss: 3159.0173 \n",
      "1440. Epoch Loss: 3159.0171 \n",
      "1480. Epoch Loss: 3159.0171 \n",
      "1520. Epoch Loss: 3159.0171 \n",
      "1560. Epoch Loss: 3159.0171 \n",
      "1600. Epoch Loss: 3159.0171 \n",
      "1640. Epoch Loss: 3159.0171 \n",
      "1680. Epoch Loss: 3159.0171 \n",
      "1720. Epoch Loss: 3159.0171 \n",
      "1760. Epoch Loss: 3159.0171 \n",
      "1800. Epoch Loss: 3159.0171 \n",
      "1840. Epoch Loss: 3159.0171 \n",
      "1880. Epoch Loss: 3159.0171 \n",
      "1920. Epoch Loss: 3159.0171 \n",
      "1960. Epoch Loss: 3159.0171 \n",
      "2000. Epoch Loss: 3159.0171 \n",
      "2040. Epoch Loss: 3159.0171 \n",
      "2080. Epoch Loss: 3159.0171 \n",
      "2120. Epoch Loss: 3159.0171 \n",
      "2160. Epoch Loss: 3159.0171 \n",
      "2200. Epoch Loss: 3159.0171 \n",
      "2240. Epoch Loss: 3159.0171 \n",
      "2280. Epoch Loss: 3159.0171 \n",
      "2320. Epoch Loss: 3159.0171 \n",
      "2360. Epoch Loss: 3159.0171 \n",
      "2400. Epoch Loss: 3159.0171 \n",
      "2440. Epoch Loss: 3159.0171 \n",
      "2480. Epoch Loss: 3159.0171 \n",
      "2520. Epoch Loss: 3159.0171 \n",
      "2560. Epoch Loss: 3159.0171 \n",
      "2600. Epoch Loss: 3159.0171 \n",
      "2640. Epoch Loss: 3159.0171 \n",
      "2680. Epoch Loss: 3159.0171 \n",
      "2720. Epoch Loss: 3159.0171 \n",
      "2760. Epoch Loss: 3159.0171 \n",
      "2800. Epoch Loss: 3159.0171 \n",
      "2840. Epoch Loss: 3159.0171 \n",
      "2880. Epoch Loss: 3159.0171 \n",
      "2920. Epoch Loss: 3159.0171 \n",
      "2960. Epoch Loss: 3159.0171 \n",
      "3000. Epoch Loss: 3159.0171 \n",
      "3040. Epoch Loss: 3159.0171 \n",
      "3080. Epoch Loss: 3159.0171 \n",
      "3120. Epoch Loss: 3159.0171 \n",
      "3160. Epoch Loss: 3159.0171 \n",
      "3200. Epoch Loss: 3159.0171 \n",
      "3240. Epoch Loss: 3159.0171 \n",
      "3280. Epoch Loss: 3159.0171 \n",
      "3320. Epoch Loss: 3159.0171 \n",
      "3360. Epoch Loss: 3159.0171 \n",
      "3400. Epoch Loss: 3159.0171 \n",
      "3440. Epoch Loss: 3159.0171 \n",
      "3480. Epoch Loss: 3159.0171 \n",
      "3520. Epoch Loss: 3159.0171 \n",
      "3560. Epoch Loss: 3159.0171 \n",
      "3600. Epoch Loss: 3159.0171 \n",
      "3640. Epoch Loss: 3159.0171 \n",
      "3680. Epoch Loss: 3159.0171 \n",
      "3720. Epoch Loss: 3159.0171 \n",
      "3760. Epoch Loss: 3159.0171 \n",
      "3800. Epoch Loss: 3159.0171 \n",
      "3840. Epoch Loss: 3159.0171 \n",
      "3880. Epoch Loss: 3159.0171 \n",
      "3920. Epoch Loss: 3159.0171 \n",
      "3960. Epoch Loss: 3159.0171 \n",
      "4000. Epoch Loss: 3159.0171 \n",
      "4040. Epoch Loss: 3159.0171 \n",
      "4080. Epoch Loss: 3159.0171 \n",
      "4120. Epoch Loss: 3159.0171 \n",
      "4160. Epoch Loss: 3159.0171 \n",
      "4200. Epoch Loss: 3159.0171 \n",
      "4240. Epoch Loss: 3159.0171 \n",
      "4280. Epoch Loss: 3159.0171 \n",
      "4320. Epoch Loss: 3159.0171 \n",
      "4360. Epoch Loss: 3159.0171 \n",
      "4400. Epoch Loss: 3159.0171 \n",
      "4440. Epoch Loss: 3159.0171 \n",
      "4480. Epoch Loss: 3159.0171 \n",
      "4520. Epoch Loss: 3159.0171 \n",
      "4560. Epoch Loss: 3159.0171 \n",
      "4600. Epoch Loss: 3159.0171 \n",
      "4640. Epoch Loss: 3159.0171 \n",
      "4680. Epoch Loss: 3159.0171 \n",
      "4720. Epoch Loss: 3159.0171 \n",
      "4760. Epoch Loss: 3159.0171 \n",
      "4800. Epoch Loss: 3159.0171 \n",
      "4840. Epoch Loss: 3159.0171 \n",
      "4880. Epoch Loss: 3159.0171 \n",
      "4920. Epoch Loss: 3159.0171 \n",
      "4960. Epoch Loss: 3159.0171 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7c93d34bc889>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             }\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m#writer.add_summary(merged, epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m40\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jimmy/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jimmy/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jimmy/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/Jimmy/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jimmy/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init =  tf.initialize_all_variables()\n",
    "\n",
    "num_epochs=10000\n",
    "#merge = tf.merge_all_summaries()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    writer = tf.train.SummaryWriter(\"./summary\", sess.graph)\n",
    "    sess.run(init)\n",
    "    #sess.run(merge)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(X.shape[0]//batch_size):\n",
    "            feed_dict = {\n",
    "                x : X[batch*batch_size:batch*batch_size + batch_size],\n",
    "            }\n",
    "            _, lossVal = sess.run([train_step, loss], feed_dict=feed_dict)\n",
    "        #writer.add_summary(merged, epoch)\n",
    "        if(epoch%40 == 0):\n",
    "            print(\"%d. Epoch Loss: %.4f \"  %(epoch, lossVal))\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
